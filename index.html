<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="NeRF-MS: Neural Radiance Fields with Multi-Sequence">
    <meta name="author" content="Peihao Li,
                                Shaohui Wang,
                                Chen Yang,
                                Bingbing Liu,
								Weichao Qiu,
                                Haoqian Wang">

    <title>NeRF-MS: Neural Radiance Fields with Multi-Sequence</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="./static/css/offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>NeRF-MS: Neural Radiance Fields with Multi-Sequence</h2>
        <h3>ICCV 2023</h3>
    <hr>
    <p class="authors">
        <a href="https://scholar.google.com/citations?user=-joTlVUAAAAJ&hl=en"> Peihao Li</a>,
        <a > Shaohui Wang</a>,
        <a href="https://scholar.google.com/citations?user=StdXTR8AAAAJ&hl=en"> Chen Yang</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=-rCulKwAAAAJ"> Bingbing Liu</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=9_AUwFUAAAAJ"> Weichao Qiu</a>,
        <a href="https://scholar.google.com/citations?user=eldgnIYAAAAJ&hl=en"> Haoqian Wang</a>,
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_NeRF-MS_Neural_Radiance_Fields_with_Multi-Sequence_ICCV_2023_paper.pdf">Paper</a>
        <a class="btn btn-primary" href="./static/data/2323_NeRFMS_poster.pdf">Poster</a>
        <a class="btn btn-primary" href="https://gitee.com/nerfms/nerfms_mindspore">Code</a>

    </div>
</div>

<div class="container">
    <div class="section">
       
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="static/img/teaser.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
        <p>
            Neural radiance fields (NeRF) achieve impressive performance in novel view synthesis when trained on only single sequence data. 
            However, leveraging multiple sequences captured by different cameras at different times is essential for better reconstruction 
            performance. Multi-sequence data takes two main challenges: appearance variation due to different lighting conditions and 
            non-static objects like pedestrians. To address these issues, we propose NeRF-MS, a novel approach to training NeRF with 
            multi-sequence data. Specifically, we utilize a triplet loss to regularize the distribution of per-image appearance code, which 
            leads to better high-frequency texture and consistent appearance, such as specular reflections. Then, we explicitly model non-static
             objects to reduce floaters. Extensive results demonstrate that NeRF-MS not only outperforms state-of-the-art view synthesis 
             methods on outdoor and synthetic scenes, but also achieves 3D consistent rendering and robust appearance controlling.

        </p>
    </div>


    <div class="section">
        <h2>Pipeline</h2>
        <hr>
        <p>
            Our appearance variation module outputs the static color based on the per-image appearance code. 
            We use <b>triplet loss</b> to regularize the distribution of appearance code. By utilizing the image transient code and <b>sequence transient code</b>, the transient decomposition module can effectively generate the color, density, and uncertainty for non-static objects. The static and transient components are then integrated to obtain the pixel's color and uncertainty. Finally, we employ a color loss to supervise the radiance field.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="static/img/pipeline.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
    </div>

    <div class="section">
        <h2>Ablation</h2>
        <hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="static/img/appearance_aba.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>
        <p>
            <b>(a)</b> Without triplet loss, the appearance codes from different sequences are overlapped due to overfitting.
            <b>(b)</b> Our method can
            reconstruct fine details (window dividers) and 3D consistent
            reflection (on windows and bulldozer tracks) by utilizing
            triplet loss to prevent appearance code overfitting.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="static/img/transient_aba.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>
        <p>
            In the provided GT diagram, we label sequence transient region as yellow and image transient region as green. 
            Subsequently, we present the decomposition performance of various algorithms alongside their corresponding depth maps. 
            Our proposed method addresses the challenging yellow region that SOTA methods struggle with, but also  preserves the intricate geometric details of the static region, resulting in more accurate depth estimation.
        </p>
    </div>

    

    <div class="section">
        <h2>Results</h2>
        <hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="static/img/results.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>
        

        <p>
            Our method significantly improves the rendering performance in test views with multiple sequences as training data. What's more, we achieve robust appearance interpolation as below:
        </p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="static/img/interpolation.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>

    </div>

    

   
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
  @InProceedings{Li_2023_ICCV,
  author    = {Li, Peihao and Wang, Shaohui and Yang, Chen and Liu, Bingbing and Qiu, Weichao and Wang, Haoqian},
  title     = {NeRF-MS: Neural Radiance Fields with Multi-Sequence},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2023},
  pages     = {18591-18600}
  }
        </div>
    </div>

    <footer>
        <p><b>Acknowledgements:</b> The website template was borrowed from <a href="https://apchenstu.github.io/">Anpei Chen</a>. <br>Send feedback and questions to Peihao Li, lph21@mails.tsinghua.edu.cn</p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
